# Job Configuration
#   An Aztk Job is a cluster and an array of Spark applications to run on that cluster
#   AZTK Spark Jobs will automatically manage the lifecycle of the cluster
#   For more information see the documentation at: http://aztk.readthedocs.io/en/latest/70-jobs.html

job:
    cluster_configuration:
        worker_on_master: true
        vm_size: Standard_D13_v2
        size: 0
        size_low_priority: 10
        toolkit:
          software: spark
          version: 2.3.0
          docker_repo: atgenomix/seqslab_runtime_1.0
          # docker_run_options: --shm-size="10g" --device /dev/fuse --privileged --env TEST="test"

    spark_configuration:
        spark_defaults_conf: spark-defaults.error_correction.conf
        spark_env_sh: spark-env.sh
        core_site_xml: core-site.xml

    # an application maps directly to a spark-submit command
    applications:
      - name: error_correction
        application: /${JAR_FOLDER}/connectedreads-1.0.0.jar
        application_args:
          - correct
          - ${INPUT_FOLDER}
          - ${OUTPUT_FOLDER}
          - -keep_err
          - -pl_batch
          - 2
          - -pl_partition
          - 6
          - -mlcp
          - 45
          - -max_read_length
          - 160
          - -mim_err_depth
          - 40
          - -min_read_support
          - 3
          - -max_correction_ratio
          - 0.5
          - -max_err_read
          - 1
          - -raw_err_group_len
          - 1
          - -raw_err_cutoff
          - 1
          - -seperate_err
        main_class: com.atgenomix.connectedreads.cli.GraphSeqMain
