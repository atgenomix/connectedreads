# Job Configuration
#   An Aztk Job is a cluster and an array of Spark applications to run on that cluster
#   AZTK Spark Jobs will automatically manage the lifecycle of the cluster
#   For more information see the documentation at: http://aztk.readthedocs.io/en/latest/70-jobs.html

job:
    cluster_configuration:
        worker_on_master: true
        vm_size: Standard_D13_v2
        size: 0
        size_low_priority: 10
        toolkit:
          software: spark
          version: 2.3.0
          docker_repo: atgenomix/seqslab_runtime_1.0
          # docker_run_options: --shm-size="10g" --device /dev/fuse --privileged --env TEST="test"

    spark_configuration:
        spark_defaults_conf: spark-defaults.string_graph.conf
        spark_env_sh: spark-env.sh
        core_site_xml: core-site.xml

    # an application maps directly to a spark-submit command
    applications:
      - name: string_graph
        application: /${JAR_FOLDER}/connectedreads-1.0.0.jar
        application_args:
          - overlap
          - ${INPUT_FOLDER}
          - ${OUTPUT_FOLDER}
          - -pl_batch
          - 1
          - -pl_partition
          - 7
          - -mlcp
          - 85
          - -max_read_length
          - 160
          - -rmdup
          - -cache
          - -checkpoint_path
          - ${TMP_FOLDER}
        main_class: com.atgenomix.connectedreads.cli.GraphSeqMain
