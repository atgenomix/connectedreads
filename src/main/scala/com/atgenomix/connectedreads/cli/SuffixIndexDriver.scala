/**
  * Copyright (C) 2015, Atgenomix Incorporated. All Rights Reserved.
  * This program is an unpublished copyrighted work which is proprietary to
  * Atgenomix Incorporated and contains confidential information that is not to
  * be reproduced or disclosed to any other person or entity without prior
  * written consent from Atgenomix, Inc. in each and every instance.
  * Unauthorized reproduction of this program as well as unauthorized
  * preparation of derivative works based upon the program or distribution of
  * copies by sale, rental, lease or lending are violations of federal copyright
  * laws and state trade secret laws, punishable by civil and criminal penalties.
  */

package com.atgenomix.connectedreads.cli

import java.io._

import com.atgenomix.connectedreads.core.model.NumberedReads
import com.atgenomix.connectedreads.core.util.SequenceUtils._
import com.atgenomix.connectedreads.io.SuffixTreeWriter
import com.atgenomix.connectedreads.pipeline.SuffixIndexPipeline
import org.apache.spark.sql.SparkSession
import org.apache.spark.storage.StorageLevel
import org.bdgenomics.utils.cli.{Args4j, Args4jBase}
import org.kohsuke.args4j.{Argument, Option => Args4jOption}

object SuffixIndexDriver extends CommandCompanion with Serializable {
  override val commandName: String = "index"
  override val commandDescription: String = "Suffix indexing Generation"

  override def apply(cmdLine: Array[String]): Command = new SuffixIndexDriver(Args4j[SuffixIndexArgs](cmdLine))
}

class SuffixIndexArgs extends Args4jBase with Serializable {
  @Argument(required = true, metaVar = "INPUT", usage = "Input path (generated by Adam transform)", index = 0)
  var input: String = _

  @Argument(required = true, metaVar = "OUTPUT", usage = "Output path", index = 1)
  var output: String = _

  @Args4jOption(required = true, name = "-pl_batch", usage = "Prefix length for number of batches [default=1]")
  var pl: Short = 1

  @Args4jOption(required = true, name = "-pl_partition", usage = "Prefix length for number of partitions [default=7]")
  var pl2: Short = 7

  @Args4jOption(required = false, name = "-rmdup", usage = "Remove duplication of reads")
  var rmdup: Boolean = false
  
  @Args4jOption(required = false, name = "-numbering", usage = "Assign an unique number for each read automatically")
  var numbering_enable: Boolean = false

  @Args4jOption(required = false, name = "-packing_size", usage = "The number of reads will be packed together [default = 100]")
  var packing_size: Int = 100

  @Args4jOption(required = false, name = "-mlcp", usage = "Minimal longest common prefix [default = 45]")
  var minlcp: Int = 45

  @Args4jOption(required = false, name = "-max_read_length", usage = "Maximal read length [default = 151]")
  var maxrlen: Int = 151
  
  @Args4jOption(required = false, name = "-max_N_count", usage = "upper limit for uncalled base count")
  var maxNCount: Int = 10

  @Args4jOption(required = false, name = "-output_format", usage = "index output format [AVRO]")
  var outputFormat: String = "AVRO"
}

class SuffixIndexDriver(val args: SuffixIndexArgs) extends Serializable with SparkCommand[SuffixIndexArgs] {
  override val companion: CommandCompanion = SuffixIndexDriver
  
  val ID_SHARDING_SIZE = 2000000 //read naming by idx
  val MAX_PARTITIONS = 1000 // ID_SHARDING_SIZE * MAX_PARTITIONS < 2^31
  
  override def run(spark: SparkSession): Unit = {
    import spark.implicits._
    
    val batches: Int = scala.math.pow(4, args.pl).toInt
    val columns = Seq("readName", "sequence")
    var alignments = spark
      .read
      .option("mergeSchema", "false")
      .parquet(args.input)
      .select(columns.head, columns.tail: _*)
  
    var num_partitions = alignments
      .rdd
      .getNumPartitions
    
    if (num_partitions > MAX_PARTITIONS) {
      num_partitions = MAX_PARTITIONS
    }
    
    if (args.rmdup) {
      alignments = alignments
        .distinct()
        .repartition(num_partitions)
    }
    else if (num_partitions == MAX_PARTITIONS) {
      alignments = alignments
        .repartition(num_partitions)
    }
    
    val numberedReads = NumberedReads
      .normalize(alignments, args.numbering_enable, ID_SHARDING_SIZE, spark)
      .mapPartitions(assignNBase(_, args.maxNCount))
      .persist(StorageLevel.MEMORY_AND_DISK_SER)
    
    val pipeline = new SuffixIndexPipeline(spark, args)

    for (i <- 0 until batches) {
      SuffixTreeWriter((args.pl + args.pl2).toShort, spark)
        .avro(args.output,
          pipeline
            .buildSA(numberedReads, i)
            .mapPartitions(pipeline.generateSA)
            .mapPartitions(pipeline.generateST)
            .toDF()
        )
    }
  }
}
