/**
  * Copyright (C) 2015, Atgenomix Incorporated. All Rights Reserved.
  * This program is an unpublished copyrighted work which is proprietary to
  * Atgenomix Incorporated and contains confidential information that is not to
  * be reproduced or disclosed to any other person or entity without prior
  * written consent from Atgenomix, Inc. in each and every instance.
  * Unauthorized reproduction of this program as well as unauthorized
  * preparation of derivative works based upon the program or distribution of
  * copies by sale, rental, lease or lending are violations of federal copyright
  * laws and state trade secret laws, punishable by civil and criminal penalties.
  */

package com.atgenomix.connectedreads.cli

import java.io._

import com.atgenomix.connectedreads.pipeline.AssemblyPipeline
import org.apache.spark.sql.SparkSession
import org.bdgenomics.utils.cli.{Args4j, Args4jBase}
import org.kohsuke.args4j.{Argument, Option => Args4jOption}

object AssemblyDriver extends CommandCompanion with Serializable {
  override val commandName: String = "assemble"
  override val commandDescription: String = "assemble Illumina whole reads via read overlap, read pair, barcode index"

  override def apply(cmdLine: Array[String]): Command = new AssemblyDriver(Args4j[AssemblyArgs](cmdLine))
}

class AssemblyArgs extends Args4jBase with Serializable
{
  @Argument(required = true, metaVar = "VERTEX_INPUT", usage = "Vertex input path (generated by overlap pipeline)", index = 0)
  var vertexInput: String = _

  @Argument(required = true, metaVar = "EDGE_INPUT", usage = "Edge input path (generated by overlap pipeline)", index = 1)
  var edgeInput: String = _

  @Argument(required = true, metaVar = "OUTPUT", usage = "Output path", index = 2)
  var output: String = _

  @Argument(required = true, metaVar = "CHECKPOINT_PATH", usage = "Checkpoint path", index = 3)
  var checkpointPath: String = _

  @Args4jOption(required = false, name = "-input_format", usage = "input format of vertices/edges [ASQG | PARQUET]")
  var inputFormat: String = "PARQUET"

  @Args4jOption(required = false, name = "-small_steps", usage = "Small steps [default=5]")
  var smallSteps: Int = 5

  @Args4jOption(required = false, name = "-big_steps", usage = "Big steps [default=400]")
  var bigSteps: Int = 400

  @Args4jOption(required = false, name = "-mod", usage = "Modulo for choosing vertices with label B [default=3]")
  var mod: Int = 3

  @Args4jOption(required = false, name = "-ee", usage = "Bloom filter expected elements [default=40]")
  var expectedElements: Int = 40

  @Args4jOption(required = false, name = "-fpp", usage = "Bloom filter false positive rate [default=0.001]")
  var falsePositiveRate: Double = 0.001

  @Args4jOption(required = false, name = "-denoise_len", usage = "Denoise by contig length")
  var denoiseLen: Int = 151

  @Args4jOption(required = false, name = "-intersection", usage = "Bloom filter intersection")
  var intersection: Long = 1

  @Args4jOption(required = false, name = "-contig_upper_bound", usage = "Contig upper bound")
  var contigUpperBound: Int = 10

  @Args4jOption(required = false, name = "-overlap", usage = "Overlap length")
  var overlapLen: Int = 0

  @Args4jOption(required = false, name = "-ploidy", usage = "Ploidy of the sample, 2 for human")
  var ploidy: Int = 2

  @Args4jOption(required = false, name = "-degree_profiling", usage = "Degree profiling")
  var degreeProfiling: Boolean = true

  @Args4jOption(required = false, name = "-one_to_one_profiling", usage = "One-to-one profiling")
  var oneToOneProfiling: Boolean = false

  @Args4jOption(required = false, name = "-partition", usage = "Pairing partition [default=2100]")
  var partition: Int = 2100
}

class AssemblyDriver(val args: AssemblyArgs) extends Serializable with SparkCommand[AssemblyArgs] {
  override val companion: CommandCompanion = AssemblyDriver

  override def run(spark: SparkSession): Unit = {
    implicit val session = spark
    session.sparkContext.setCheckpointDir(args.checkpointPath)

    val pipeline = new AssemblyPipeline(spark, args)
    pipeline.assemble()
  }
}
