/**
  * Copyright (C) 2015, Atgenomix Incorporated. All Rights Reserved.
  * This program is an unpublished copyrighted work which is proprietary to
  * Atgenomix Incorporated and contains confidential information that is not to
  * be reproduced or disclosed to any other person or entity without prior
  * written consent from Atgenomix, Inc. in each and every instance.
  * Unauthorized reproduction of this program as well as unauthorized
  * preparation of derivative works based upon the program or distribution of
  * copies by sale, rental, lease or lending are violations of federal copyright
  * laws and state trade secret laws, punishable by civil and criminal penalties.
  */

package com.atgenomix.connectedreads.cli

import java.io._
import java.nio.file.Paths

import com.atgenomix.connectedreads.core.algorithm.StringGraph
import com.atgenomix.connectedreads.core.model.NumberedReads
import com.atgenomix.connectedreads.core.util.QualUtils.{decodeQual, mergeQualAndDepth}
import com.atgenomix.connectedreads.core.util.SequenceUtils._
import com.atgenomix.connectedreads.io.Vertex
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.sql.{DataFrame, Dataset, Encoders, SparkSession}
import org.apache.spark.sql.functions._
import org.bdgenomics.utils.cli.{Args4j, Args4jBase}
import org.kohsuke.args4j.{Argument, Option => Args4jOption}

object StringGraphDriver extends CommandCompanion with Serializable {
  override val commandName: String = "overlap"
  override val commandDescription: String = "String Graph Generation"

  override def apply(cmdLine: Array[String]): Command = new StringGraphDriver(Args4j[StringGraphArgs](cmdLine))
}

class StringGraphArgs extends Args4jBase with Serializable {
  @Argument(required = true, metaVar = "INPUT", usage = "Input path (generated by Adam transform)", index = 0)
  var input: String = _

  @Argument(required = true, metaVar = "OUTPUT", usage = "Output path", index = 1)
  var output: String = _

  @Args4jOption(required = true, name = "-pl_batch", usage = "Prefix length for number of batches [default=1]")
  var pl: Short = 1

  @Args4jOption(required = true, name = "-pl_partition", usage = "Prefix length for number of partitions [default=7]")
  var pl2: Short = 7

  @Args4jOption(required = false, name = "-rmdup", usage = "Remove duplication of reads")
  var rmdup: Boolean = false

  @Args4jOption(required = false, name = "-cache", usage = "Cache the reads in memory to speedup data processing")
  var cached: Boolean = false

  @Args4jOption(required = false, name = "-checkpoint_path", usage = "Checkpoint path",
    depends = { Array[String]("-cache") })
  var checkpoint_path: String = _

  @Args4jOption(required = false, name = "-numbering", usage = "Assign an unique number for each read automatically")
  var numbering_enable: Boolean = false

  @Args4jOption(required = false, name = "-packing_size", usage = "The number of reads will be packed together [default = 100]")
  var packing_size: Int = 100

  @Args4jOption(required = false, name = "-mlcp", usage = "Minimal longest common prefix [default = 45]")
  var minlcp: Int = 45

  @Args4jOption(required = false, name = "-max_read_length", usage = "Maximal read length [default = 151]")
  var maxrlen: Int = 151

  @Args4jOption(required = false, name = "-max_edges", usage = "Maximal number of edges per read [default = Integer.MAX_VALUE]")
  var maxrs: Int = Integer.MAX_VALUE

  @Args4jOption(required = false, name = "-stats", usage = "Enable to output statistics of String Graph to $OUTPUT/STATS")
  var stats: Boolean = false

  @Args4jOption(required = false, name = "-profiling", usage = "Enable performance profiling and output to $OUTPUT/STATS")
  var profiling: Boolean = false

  @Args4jOption(required = false, name = "-output_format", usage = "output vertices and edges in format of ASQG | PARQUET")
  var outputFormat: String = "PARQUET"

  @Args4jOption(required = false, name = "-output_fastq", usage = "dump fastq from vertices")
  var outputFastq: Boolean = false
}

case class Alignment(readName: String, sequence: String, qual: String, rc: Int)

class StringGraphDriver(val args: StringGraphArgs) extends Serializable with SparkCommand[StringGraphArgs] {
  override val companion: CommandCompanion = StringGraphDriver
  val _vtFolder = "VT"
  val _shortDupVtFolder = "SHORT_DUP_VT"
  val _edFolder = "ED"
  val _tmpFolder = "TMP"
  val _statisticFolder = "STATS"
  val _fqFolder = "FASTQ"

  // the number of bit used to do read name encoding
  val _namingBitLen = 40
  val _nameMask = 0x000000FFFFFFFFFFL
  val _offsetMask = 0x00FFFF0000000000L
  val _rcMask = 0x4000000000000000L

  def duplicationRemoval(rmdup: Boolean,
    cached: Boolean,
    minlcp: Int,
    numbering_enable: Boolean,
    sg: StringGraph,
    alignments: DataFrame,
    rcAlignments: DataFrame,
    spark: SparkSession): Dataset[NumberedReads] = {

    if (rmdup) {
      //NOTE: need to repartition due to only 200 partitions after distinct() (might be over 1M records per partition)
      import spark.implicits._
      val encoder = Encoders.product[Alignment]
      val rcDistinctAlignments = (alignments union rcAlignments).as[Alignment](encoder)
        .groupByKey(_.sequence)
        .mapGroups{ case (_, xs) =>
          val alignments = xs.toList
          val decodedQual = alignments.map(i => decodeQual(i.qual))
          val mergedQual = mergeQualAndDepth(decodedQual)
          alignments.minBy(_.readName).copy(qual = mergedQual)
        }
        .filter(_.rc == 0)
        .toDF()
        .select("readName", "sequence", "qual")
      if (cached) {
        val numberedReads = sg.numbering(rcDistinctAlignments, minlcp, numbering_enable).cache()
        //TODO: task failed when adding '--conf "spark.sql.shuffle.partitions=1000"'
        //val numberedReads = sg.numbering(alignments.distinct()).cache()
        val cachedReads = numberedReads.checkpoint()
        numberedReads.unpersist()
        cachedReads
      } else {
        val numberedReads = sg.numbering(rcDistinctAlignments, minlcp, numbering_enable)
        //TODO: task failed when adding '--conf "spark.sql.shuffle.partitions=1000"'
        //val numberedReads = sg.numbering(alignments.distinct()).cache()
        numberedReads
      }
    } else {
      if (cached) {
        val numberedReads = sg.numbering(alignments, minlcp, numbering_enable).cache()
        val cachedReads = numberedReads.checkpoint()
        numberedReads.unpersist()
        cachedReads
      } else {
        val numberedReads = sg.numbering(alignments, minlcp, numbering_enable)
        numberedReads
      }
    }
  }

  def printstat(statsF: BufferedWriter, stats: Boolean, content: String): Unit = {
    if (stats) {
      statsF.write(content)
    }
  }

  override def run(spark: SparkSession): Unit = {
    val sg = new StringGraph(spark, args, _namingBitLen, _nameMask, _offsetMask, _rcMask)
    val batches: Int = scala.math.pow(4, args.pl).toInt
    val columnNames = Seq("readName", "sequence", "qual")
    val alignments = spark.read.option("mergeSchema", "false")
      .parquet(args.input)
      .select(columnNames.head, columnNames.tail: _*)
      .withColumn("rc", lit(0))
    val rcUdf = udf((xs: String) => reverseComplementary(xs))
    val rcAlignments = alignments.withColumn("sequence", rcUdf(col("sequence")))
      .withColumn("rc", lit(1))
    if (args.cached) {
      spark.sparkContext.setCheckpointDir(args.checkpoint_path)
    }

    var numberedReads = duplicationRemoval(args.rmdup, args.cached, args.minlcp, args.numbering_enable, sg, alignments, rcAlignments, spark)
    val conf = new Configuration
    conf.addResource(new Path("/usr/local/hadoop/etc/hadoop/core-site.xml"))
    val fs: FileSystem = FileSystem.get(conf)

    // create stats-summary file
    val statsSumPath = "stats-summary"
    val statsPath = Paths.get(args.output, _statisticFolder).toString
    val statsF = {
      if (args.stats) {
        new BufferedWriter(
          new OutputStreamWriter(
            fs.create(new Path(Paths.get(statsPath, statsSumPath).toString), true)
          )
        )
      } else {
        null
      }
    }

    // create profiling-summary file
    val profileingSumPath = "profiling-summary"
    val profilingF = {
      if (args.profiling) {
        new BufferedWriter(
          new OutputStreamWriter(
            fs.create(new Path(Paths.get(statsPath, profileingSumPath).toString), true)
          )
        )
      } else {
        null
      }
    }

    // write short duplicated vertex information
    for (i <- 0 until batches) {
      val path = Paths.get(args.output, _shortDupVtFolder, i.toString).toString
      val data = sg.preprocessing(i, numberedReads)
      sg.findShortDupReads(data, i, statsPath)
        .write.parquet(path)
    }

    // read short duplicated vertex information
    val shortDupVt = spark.read.parquet(Paths.get(args.output, _shortDupVtFolder, "*").toString)

    // write vertex information
    import spark.implicits._
    val o = Paths.get(args.output, _vtFolder).toString
    val encoder = Encoders.product[NumberedReads]
    var deDupNumberedReads = numberedReads.join(shortDupVt, Seq("sn"), "leftanti")
        .as[NumberedReads](encoder)
        .cache()

    if (args.outputFormat == "ASQG")
      deDupNumberedReads.map(x => "VT\t" + x.sn.toString + "\t" + x.sequence + "\t" + x.qual + "\tSS:i:0")
        .write.text(o)
    else
      deDupNumberedReads.map(x => Vertex(x.sn, x.sequence, x.qual))
        .toDF
        .write.parquet(o)

    // dump fastq with vertex information
    if (args.outputFastq)
      deDupNumberedReads
        .map(x => "@" + x.sn.toString + "\n" + x.sequence + "\n+\n" + x.qual)
        .write.text(Paths.get(args.output, _fqFolder).toString)

    // print vertex stats
    printstat(statsF, args.stats, "vertex count " + deDupNumberedReads.count() + "\n")

    for (i <- 0 until batches) {
      if (args.outputFormat == "ASQG") {
        // TODO: filter out unnecessary vertices
        deDupNumberedReads = spark.read.text(o).map { x =>
          val items = x.toString().split("\t")
          NumberedReads(items(1).toLong, items(2), items(3))
        }
      }

      val smout = Paths.get(args.output, _edFolder, i.toString).toString
      val tmp = Paths.get(args.output, _tmpFolder, i.toString).toString
      val tmp_folder = new Path(tmp)
      val data = sg.preprocessing(i, deDupNumberedReads)
      sg.overlap(data, i, smout, tmp, statsPath)
      fs.delete(tmp_folder, true)
    }

    // stats information aggregation
    if (args.stats) {
      val content = new StringBuilder
      content.append("#bathID\tpartitionID\tsuffix count\tedge count\n")
      spark.sparkContext.textFile(Paths.get(statsPath, "stats").toString).collect.foreach(v => content.append(v + '\n'))
      statsF.write(content.toString)
      statsF.flush()
      statsF.close()
      fs.delete(new Path(Paths.get(statsPath, "stats").toString), true)
    }

    // profiling information aggregation
    if (args.profiling) {
      val content = new StringBuilder
      spark.sparkContext.textFile(Paths.get(statsPath, "profiling").toString).collect.foreach(v => content.append(v + '\n'))
      profilingF.write(content.toString)
      profilingF.flush()
      profilingF.close()
      fs.delete(new Path(Paths.get(statsPath, "profiling").toString), true)
    }
  }
}
